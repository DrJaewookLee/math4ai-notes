{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68cafefc",
   "metadata": {},
   "source": [
    "# Ch 9 Optimization Algorithms\n",
    "\n",
    "Textbook: \"Mathematical Methods for Artificial Intelligence\" Chapter 9\n",
    "\n",
    "This notebook covers unconstrained optimization problems and various optimization algorithms.\n",
    "We primarily use quadratic functions to analyze algorithm characteristics and visualizations.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/math4ai-notes/blob/main/Notebooks/ch09_Optimization_Algorithms.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "gh06nihbeoq",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:58:04.237314Z",
     "iopub.status.busy": "2025-08-28T16:58:04.236296Z",
     "iopub.status.idle": "2025-08-28T16:58:05.230120Z",
     "shell.execute_reply": "2025-08-28T16:58:05.229111Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Font settings for plots\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Plot style settings\n",
    "plt.style.use('default')\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb75b2",
   "metadata": {},
   "source": [
    "## 9.1 Unconstrained Optimization\n",
    "\n",
    "### 9.1.1 Definitions and Basic Concepts\n",
    "\n",
    "**Classification of Minima:**\n",
    "- **Global minimum**: $f(x^*) \\leq f(x), \\forall x \\in \\mathbb{R}^n$\n",
    "- **Local minimum**: $\\exists \\delta > 0$ s.t. $f(x^*) \\leq f(x), \\forall x \\in B(x^*, \\delta)$\n",
    "- **Strict local minimum**: $f(x^*) < f(x), \\forall x \\in B(x^*, \\delta) \\setminus \\{x^*\\}$\n",
    "- **Isolated minimum**: Unique local minimum\n",
    "- **Critical point**: $\\nabla f(x^*) = 0$\n",
    "\n",
    "### 9.1.2 Optimality Conditions\n",
    "\n",
    "**First-order necessary condition**: If $x^*$ is a local minimum, then $\\nabla f(x^*) = 0$\n",
    "\n",
    "**Second-order necessary condition**: If $x^*$ is a local minimum, then $\\nabla f(x^*) = 0$ and $\\nabla^2 f(x^*) \\succeq 0$ (PSD)\n",
    "\n",
    "**Second-order sufficient condition**: If $\\nabla f(x^*) = 0$ and $\\nabla^2 f(x^*) \\succ 0$ (PD), then $x^*$ is a strict local minimum\n",
    "\n",
    "### 9.1.3 Analytical Solution for Quadratic Functions\n",
    "\n",
    "For quadratic function $f(x) = \\frac{1}{2}x^T Q x + b^T x + c$ (where $Q \\succ 0$), the optimal solution is:\n",
    "$$x^* = -Q^{-1}b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bs4arfwr8sh",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:58:05.233120Z",
     "iopub.status.busy": "2025-08-28T16:58:05.233120Z",
     "iopub.status.idle": "2025-08-28T16:58:05.238461Z",
     "shell.execute_reply": "2025-08-28T16:58:05.238461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quadratic function definition and analytical vs numerical solution comparison\n",
    "\n",
    "def create_quadratic_function(Q, b, c=0):\n",
    "    \"\"\"Create quadratic function using SPD matrix Q\"\"\"\n",
    "    def f(x):\n",
    "        x = np.array(x)\n",
    "        return 0.5 * x.T @ Q @ x + b.T @ x + c\n",
    "    \n",
    "    def grad_f(x):\n",
    "        x = np.array(x)\n",
    "        return Q @ x + b\n",
    "    \n",
    "    def hess_f(x):\n",
    "        return Q\n",
    "    \n",
    "    # Analytical optimal solution\n",
    "    analytical_solution = -np.linalg.solve(Q, b)\n",
    "    \n",
    "    return f, grad_f, hess_f, analytical_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ld2ztc6co8p",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, h=1e-8):\n",
    "    \"\"\"Compute numerical gradient using finite differences\"\"\"\n",
    "    x = np.array(x, dtype=float)\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_minus = x.copy()\n",
    "        x_plus[i] += h\n",
    "        x_minus[i] -= h\n",
    "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def numerical_hessian(f, x, h=1e-5):\n",
    "    \"\"\"Compute numerical Hessian using finite differences\"\"\"\n",
    "    x = np.array(x, dtype=float)\n",
    "    n = len(x)\n",
    "    hess = np.zeros((n, n))\n",
    "    \n",
    "    f_x = f(x)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            x_ij = x.copy()\n",
    "            x_i = x.copy()\n",
    "            x_j = x.copy()\n",
    "            \n",
    "            x_ij[i] += h\n",
    "            x_ij[j] += h\n",
    "            x_i[i] += h\n",
    "            x_j[j] += h\n",
    "            \n",
    "            hess[i, j] = (f(x_ij) - f(x_i) - f(x_j) + f_x) / (h * h)\n",
    "    \n",
    "    return hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "yibmvmvaqr8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.1.3 Analytical vs Numerical Solutions for Quadratic Functions\n",
      "============================================================\n",
      "Function 1: f(x) = 0.5*x^T*Q1*x + b1^T*x, cond(Q1) = 10.0\n",
      "Analytical optimum: x* = [-2.000000, 0.400000]\n",
      "Function value: f(x*) = -2.80000000\n",
      "\n",
      "Function 2: f(x) = 0.5*x^T*Q2*x + b2^T*x, cond(Q2) = 1000.0\n",
      "Analytical optimum: x* = [-3.000000, 1.000000]\n",
      "Function value: f(x*) = -504.50000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create example quadratic functions with different condition numbers\n",
    "print(\"9.1.3 Analytical vs Numerical Solutions for Quadratic Functions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function 1: Condition number 10\n",
    "eigenvals1 = [1.0, 10.0]\n",
    "Q1 = np.diag(eigenvals1)\n",
    "b1 = np.array([2.0, -4.0])\n",
    "f1, grad_f1, hess_f1, analytical_opt1 = create_quadratic_function(Q1, b1)\n",
    "\n",
    "# Function 2: Condition number 1000  \n",
    "eigenvals2 = [1.0, 1000.0]\n",
    "Q2 = np.diag(eigenvals2)\n",
    "b2 = np.array([3.0, -1000.0])\n",
    "f2, grad_f2, hess_f2, analytical_opt2 = create_quadratic_function(Q2, b2)\n",
    "\n",
    "print(f\"Function 1: f(x) = 0.5*x^T*Q1*x + b1^T*x, cond(Q1) = {np.linalg.cond(Q1):.1f}\")\n",
    "print(f\"Analytical optimum: x* = [{analytical_opt1[0]:.6f}, {analytical_opt1[1]:.6f}]\")\n",
    "print(f\"Function value: f(x*) = {f1(analytical_opt1):.8f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Function 2: f(x) = 0.5*x^T*Q2*x + b2^T*x, cond(Q2) = {np.linalg.cond(Q2):.1f}\")  \n",
    "print(f\"Analytical optimum: x* = [{analytical_opt2[0]:.6f}, {analytical_opt2[1]:.6f}]\")\n",
    "print(f\"Function value: f(x*) = {f2(analytical_opt2):.8f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "w3dt8r8ixis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Differentiation Verification (Test point: [1.0, 2.0])\n",
      "------------------------------\n",
      "Function 1 - Analytical gradient: [3.000000, 16.000000]\n",
      "Function 1 - Numerical gradient:  [3.000000, 16.000000]\n",
      "Gradient error: 1.07e-07\n",
      "\n",
      "Function 1 - Analytical Hessian:\n",
      "[[ 1.  0.]\n",
      " [ 0. 10.]]\n",
      "Function 1 - Numerical Hessian:\n",
      "[[1.00000008e+00 1.77635684e-05]\n",
      " [1.77635684e-05 9.99996530e+00]]\n",
      "Hessian error: 4.28e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Numerical differentiation verification\n",
    "test_point = np.array([1.0, 2.0])\n",
    "print(\"Numerical Differentiation Verification (Test point: [1.0, 2.0])\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "analytical_grad1 = grad_f1(test_point)\n",
    "numerical_grad1 = numerical_gradient(f1, test_point)\n",
    "print(f\"Function 1 - Analytical gradient: [{analytical_grad1[0]:.6f}, {analytical_grad1[1]:.6f}]\")\n",
    "print(f\"Function 1 - Numerical gradient:  [{numerical_grad1[0]:.6f}, {numerical_grad1[1]:.6f}]\")\n",
    "print(f\"Gradient error: {np.linalg.norm(analytical_grad1 - numerical_grad1):.2e}\")\n",
    "print()\n",
    "\n",
    "analytical_hess1 = hess_f1(test_point)\n",
    "numerical_hess1 = numerical_hessian(f1, test_point)\n",
    "print(f\"Function 1 - Analytical Hessian:\")\n",
    "print(analytical_hess1)\n",
    "print(f\"Function 1 - Numerical Hessian:\")\n",
    "print(numerical_hess1)\n",
    "print(f\"Hessian error: {np.linalg.norm(analytical_hess1 - numerical_hess1):.2e}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1v1b297ezyx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimality Conditions Verification\n",
      "--------------------\n",
      "Gradient norm at optimum: ||∇f(x*)|| = 0.00e+00\n",
      "Hessian eigenvalues: [ 1. 10.]\n",
      "Is Hessian positive definite? True\n"
     ]
    }
   ],
   "source": [
    "# Optimality conditions verification\n",
    "print(\"Optimality Conditions Verification\")\n",
    "print(\"-\" * 20)\n",
    "grad_at_opt1 = grad_f1(analytical_opt1)\n",
    "hess_at_opt1 = hess_f1(analytical_opt1)\n",
    "eigenvals_hess1 = np.linalg.eigvals(hess_at_opt1)\n",
    "\n",
    "print(f\"Gradient norm at optimum: ||∇f(x*)|| = {np.linalg.norm(grad_at_opt1):.2e}\")\n",
    "print(f\"Hessian eigenvalues: {eigenvals_hess1}\")\n",
    "print(f\"Is Hessian positive definite? {np.all(eigenvals_hess1 > 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "pbwz6tng1bl",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:58:05.241471Z",
     "iopub.status.busy": "2025-08-28T16:58:05.241471Z",
     "iopub.status.idle": "2025-08-28T16:58:05.247376Z",
     "shell.execute_reply": "2025-08-28T16:58:05.246370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic optimization algorithms implementation\n",
    "\n",
    "def gradient_descent(f, grad_f, x0, learning_rate=0.01, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"Gradient descent algorithm\"\"\"\n",
    "    x = x0.copy()\n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "            \n",
    "        x_new = x - learning_rate * grad\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_f(x_new)))\n",
    "        history['step_size'].append(learning_rate)\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1w2sf7dfntk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(f, grad_f, hess_f, x0, max_iter=100, tol=1e-6):\n",
    "    \"\"\"Newton's method for optimization\"\"\"\n",
    "    x = x0.copy()\n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "            \n",
    "        hess = hess_f(x)\n",
    "        try:\n",
    "            step = np.linalg.solve(hess, grad)\n",
    "            x_new = x - step\n",
    "            step_size = np.linalg.norm(step)\n",
    "        except np.linalg.LinAlgError:\n",
    "            step = 0.01 * grad\n",
    "            x_new = x - step\n",
    "            step_size = 0.01 * grad_norm\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_f(x_new)))\n",
    "        history['step_size'].append(step_size)\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7nlaerdeqt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical vs Gradient Descent Comparison\n",
      "========================================\n",
      "\n",
      "Condition number 10.0 function:\n",
      "Analytical optimum:  x* = [-2.000000, 0.400000]\n",
      "Gradient descent result: x* = [-1.999999, 0.400000]\n",
      "Error: 9.58e-07\n",
      "Iterations: 150\n",
      "\n",
      "Condition number 1000.0 function:\n",
      "Analytical optimum:  x* = [-3.000000, 1.000000]\n",
      "Gradient descent result: x* = [-1.918401, 1.000000]\n",
      "Error: 1.08e+00\n",
      "Iterations: 2000\n"
     ]
    }
   ],
   "source": [
    "# Analytical vs Numerical solution comparison experiment\n",
    "print(\"Analytical vs Gradient Descent Comparison\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Starting point\n",
    "x0 = np.array([5.0, -3.0])\n",
    "\n",
    "# Gradient descent on function 1\n",
    "x_gd1, hist_gd1 = gradient_descent(f1, grad_f1, x0, learning_rate=0.1, max_iter=200)\n",
    "print(f\"\\nCondition number {np.linalg.cond(Q1):.1f} function:\")\n",
    "print(f\"Analytical optimum:  x* = [{analytical_opt1[0]:.6f}, {analytical_opt1[1]:.6f}]\")\n",
    "print(f\"Gradient descent result: x* = [{x_gd1[0]:.6f}, {x_gd1[1]:.6f}]\")\n",
    "print(f\"Error: {np.linalg.norm(x_gd1 - analytical_opt1):.2e}\")\n",
    "print(f\"Iterations: {len(hist_gd1['f'])-1}\")\n",
    "\n",
    "# Gradient descent on function 2\n",
    "x_gd2, hist_gd2 = gradient_descent(f2, grad_f2, x0, learning_rate=0.001, max_iter=2000)\n",
    "print(f\"\\nCondition number {np.linalg.cond(Q2):.1f} function:\")\n",
    "print(f\"Analytical optimum:  x* = [{analytical_opt2[0]:.6f}, {analytical_opt2[1]:.6f}]\")\n",
    "print(f\"Gradient descent result: x* = [{x_gd2[0]:.6f}, {x_gd2[1]:.6f}]\")\n",
    "print(f\"Error: {np.linalg.norm(x_gd2 - analytical_opt2):.2e}\")\n",
    "print(f\"Iterations: {len(hist_gd2['f'])-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10rvc1phmna",
   "metadata": {},
   "source": [
    "## 9.2 Line Search Methods\n",
    "\n",
    "### 9.2.1 Learning Rate and Convergence Speed\n",
    "\n",
    "The choice of learning rate (step size) $\\alpha$ in gradient descent significantly affects convergence speed.\n",
    "\n",
    "### 9.2.2 Wolfe and Armijo Conditions\n",
    "\n",
    "**Armijo condition** (sufficient decrease):\n",
    "$$f(x_k + \\alpha_k d_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^T d_k$$\n",
    "\n",
    "**Wolfe conditions** (adds curvature condition):\n",
    "$$|\\nabla f(x_k + \\alpha_k d_k)^T d_k| \\leq c_2 |\\nabla f(x_k)^T d_k|$$\n",
    "\n",
    "where $0 < c_1 < c_2 < 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ozrfum51rg",
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo_line_search(f, grad_f, x, direction, c1=1e-4, alpha_init=1.0, rho=0.5, max_iter=50):\n",
    "    \"\"\"\n",
    "    Armijo backtracking line search\n",
    "    \n",
    "    Parameters:\n",
    "    - f: objective function\n",
    "    - grad_f: gradient function\n",
    "    - x: current point\n",
    "    - direction: search direction\n",
    "    - c1: Armijo constant (default: 1e-4)\n",
    "    - alpha_init: initial step size (default: 1.0)\n",
    "    - rho: reduction factor (default: 0.5)\n",
    "    - max_iter: maximum iterations (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "    - alpha: step size satisfying Armijo condition\n",
    "    \"\"\"\n",
    "    alpha = alpha_init\n",
    "    fx = f(x)\n",
    "    grad_fx = grad_f(x)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        if f(x + alpha * direction) <= fx + c1 * alpha * np.dot(grad_fx, direction):\n",
    "            return alpha\n",
    "        alpha *= rho\n",
    "    \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0gd3gr6davn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe_line_search(f, grad_f, x, direction, c1=1e-4, c2=0.9, alpha_init=1.0, max_iter=50):\n",
    "    \"\"\"\n",
    "    Wolfe line search (strong Wolfe conditions)\n",
    "    \n",
    "    Parameters:\n",
    "    - f: objective function\n",
    "    - grad_f: gradient function\n",
    "    - x: current point\n",
    "    - direction: search direction\n",
    "    - c1: Armijo constant (default: 1e-4)\n",
    "    - c2: curvature constant (default: 0.9)\n",
    "    - alpha_init: initial step size (default: 1.0)\n",
    "    - max_iter: maximum iterations (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "    - alpha: step size satisfying Wolfe conditions\n",
    "    \"\"\"\n",
    "    def zoom(alpha_lo, alpha_hi):\n",
    "        for _ in range(max_iter):\n",
    "            alpha = (alpha_lo + alpha_hi) / 2\n",
    "            x_new = x + alpha * direction\n",
    "            \n",
    "            if f(x_new) > fx + c1 * alpha * grad_fx_dot_d or f(x_new) >= f(x + alpha_lo * direction):\n",
    "                alpha_hi = alpha\n",
    "            else:\n",
    "                grad_new = grad_f(x_new)\n",
    "                if abs(np.dot(grad_new, direction)) <= -c2 * grad_fx_dot_d:\n",
    "                    return alpha\n",
    "                if np.dot(grad_new, direction) * (alpha_hi - alpha_lo) >= 0:\n",
    "                    alpha_hi = alpha_lo\n",
    "                alpha_lo = alpha\n",
    "        return alpha\n",
    "    \n",
    "    alpha = alpha_init\n",
    "    fx = f(x)\n",
    "    grad_fx = grad_f(x)\n",
    "    grad_fx_dot_d = np.dot(grad_fx, direction)\n",
    "    \n",
    "    alpha_prev = 0\n",
    "    for i in range(1, max_iter + 1):\n",
    "        x_new = x + alpha * direction\n",
    "        fx_new = f(x_new)\n",
    "        \n",
    "        if fx_new > fx + c1 * alpha * grad_fx_dot_d or (i > 1 and fx_new >= f(x + alpha_prev * direction)):\n",
    "            return zoom(alpha_prev, alpha)\n",
    "        \n",
    "        grad_new = grad_f(x_new)\n",
    "        if abs(np.dot(grad_new, direction)) <= -c2 * grad_fx_dot_d:\n",
    "            return alpha\n",
    "        \n",
    "        if np.dot(grad_new, direction) >= 0:\n",
    "            return zoom(alpha, alpha_prev)\n",
    "        \n",
    "        alpha_prev = alpha\n",
    "        alpha *= 2\n",
    "    \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dko6xrghje",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test line search methods with visualization\n",
    "def test_line_search_methods():\n",
    "    \"\"\"Test and visualize line search methods\"\"\"\n",
    "    # Create a simple quadratic function for testing\n",
    "    Q_test = np.array([[2.0, 0.5], [0.5, 1.0]])\n",
    "    b_test = np.array([1.0, 1.0])\n",
    "    f_test, grad_f_test, _, _ = create_quadratic_function(Q_test, b_test)\n",
    "    \n",
    "    x_test = np.array([2.0, 2.0])\n",
    "    direction_test = -grad_f_test(x_test)  # Steepest descent direction\n",
    "    \n",
    "    # Test both methods\n",
    "    alpha_armijo = armijo_line_search(f_test, grad_f_test, x_test, direction_test)\n",
    "    alpha_wolfe = wolfe_line_search(f_test, grad_f_test, x_test, direction_test)\n",
    "    \n",
    "    print(f\"Starting point: {x_test}\")\n",
    "    print(f\"Search direction: {direction_test}\")\n",
    "    print(f\"Armijo step size: {alpha_armijo:.6f}\")\n",
    "    print(f\"Wolfe step size: {alpha_wolfe:.6f}\")\n",
    "    \n",
    "    # Visualize line search\n",
    "    alphas = np.linspace(0, 2, 100)\n",
    "    phi_values = [f_test(x_test + alpha * direction_test) for alpha in alphas]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(alphas, phi_values, 'b-', label='φ(α) = f(x + α·d)', linewidth=2)\n",
    "    plt.axvline(x=alpha_armijo, color='red', linestyle='--', label=f'Armijo: α = {alpha_armijo:.4f}')\n",
    "    plt.axvline(x=alpha_wolfe, color='green', linestyle='--', label=f'Wolfe: α = {alpha_wolfe:.4f}')\n",
    "    plt.xlabel('Step size α')\n",
    "    plt.ylabel('Function value φ(α)')\n",
    "    plt.title('Line Search Methods Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Run the test\n",
    "test_line_search_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vqjmfk7d1i9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_line_search(f, grad_f, x0, line_search_method='armijo', max_iter=1000, tol=1e-6):\n",
    "    \"\"\"Gradient descent with line search\"\"\"\n",
    "    x = x0.copy()\n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "            \n",
    "        direction = -grad  # Steepest descent direction\n",
    "        \n",
    "        # Choose line search method\n",
    "        if line_search_method == 'armijo':\n",
    "            alpha = armijo_line_search(f, grad_f, x, direction)\n",
    "        elif line_search_method == 'wolfe':\n",
    "            alpha = wolfe_line_search(f, grad_f, x, direction)\n",
    "        else:\n",
    "            alpha = 0.01  # Fixed step size fallback\n",
    "            \n",
    "        x_new = x + alpha * direction\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_f(x_new)))\n",
    "        history['step_size'].append(alpha)\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ssg7kxntki",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:58:05.250375Z",
     "iopub.status.busy": "2025-08-28T16:58:05.250375Z",
     "iopub.status.idle": "2025-08-28T16:58:05.255787Z",
     "shell.execute_reply": "2025-08-28T16:58:05.254782Z"
    }
   },
   "source": [
    "## 9.3 Optimization Algorithms\n",
    "\n",
    "### 9.3.1 BFGS and L-BFGS\n",
    "\n",
    "**BFGS (Broyden–Fletcher–Goldfarb–Shanno)**: A quasi-Newton method that iteratively updates an approximation of the Hessian using gradient information\n",
    "\n",
    "**L-BFGS (Limited-memory BFGS)**: A memory-efficient variant of BFGS that uses a two-loop recursion\n",
    "\n",
    "### 9.3.2 Momentum and Adaptive Methods\n",
    "\n",
    "**Momentum**: Uses inertia from previous update directions to reduce oscillations and accelerate convergence\n",
    "\n",
    "**Nesterov momentum**: An improved momentum method that considers the gradient at the anticipated future position\n",
    "\n",
    "**Adam**: Combines momentum with adaptive learning rates for robust optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yfcnme5gxyg",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:58:05.257788Z",
     "iopub.status.busy": "2025-08-28T16:58:05.257788Z",
     "iopub.status.idle": "2025-08-28T16:58:05.263919Z",
     "shell.execute_reply": "2025-08-28T16:58:05.263919Z"
    }
   },
   "outputs": [],
   "source": [
    "# 9.3 Optimization algorithms implementation\n",
    "\n",
    "def bfgs(f, grad_f, x0, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"BFGS quasi-Newton method\"\"\"\n",
    "    x = x0.copy()\n",
    "    n = len(x)\n",
    "    B_inv = np.eye(n)  # Initialize inverse Hessian approximation\n",
    "    \n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    grad = grad_f(x)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        \n",
    "        # Search direction\n",
    "        direction = -B_inv @ grad\n",
    "        \n",
    "        # Line search\n",
    "        alpha = armijo_line_search(f, grad_f, x, direction)\n",
    "        \n",
    "        # Update\n",
    "        x_new = x + alpha * direction\n",
    "        grad_new = grad_f(x_new)\n",
    "        \n",
    "        # BFGS update\n",
    "        s = x_new - x\n",
    "        y = grad_new - grad\n",
    "        \n",
    "        # Check curvature condition\n",
    "        if np.dot(s, y) > 1e-10:\n",
    "            rho = 1 / np.dot(y, s)\n",
    "            I = np.eye(n)\n",
    "            B_inv = (I - rho * np.outer(s, y)) @ B_inv @ (I - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_new))\n",
    "        history['step_size'].append(alpha)\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "        grad = grad_new\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mc3hj3fzqxh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs_two_loop(s_list, y_list, grad, m=10):\n",
    "    \"\"\"L-BFGS two-loop recursion\"\"\"\n",
    "    q = grad.copy()\n",
    "    alpha_list = []\n",
    "    rho_list = []\n",
    "    \n",
    "    # First loop\n",
    "    for i in range(len(s_list)-1, -1, -1):\n",
    "        s_i = s_list[i]\n",
    "        y_i = y_list[i]\n",
    "        rho_i = 1 / np.dot(y_i, s_i)\n",
    "        rho_list.append(rho_i)\n",
    "        alpha_i = rho_i * np.dot(s_i, q)\n",
    "        alpha_list.append(alpha_i)\n",
    "        q = q - alpha_i * y_i\n",
    "    \n",
    "    # Scaling\n",
    "    if len(s_list) > 0:\n",
    "        s_k = s_list[-1]\n",
    "        y_k = y_list[-1]\n",
    "        gamma = np.dot(s_k, y_k) / np.dot(y_k, y_k)\n",
    "        r = gamma * q\n",
    "    else:\n",
    "        r = q\n",
    "    \n",
    "    # Second loop\n",
    "    rho_list.reverse()\n",
    "    alpha_list.reverse()\n",
    "    \n",
    "    for i in range(len(s_list)):\n",
    "        s_i = s_list[i]\n",
    "        y_i = y_list[i]\n",
    "        rho_i = rho_list[i]\n",
    "        alpha_i = alpha_list[i]\n",
    "        beta = rho_i * np.dot(y_i, r)\n",
    "        r = r + s_i * (alpha_i - beta)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0qjpqufxs3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs(f, grad_f, x0, m=10, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"L-BFGS method\"\"\"\n",
    "    x = x0.copy()\n",
    "    s_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    grad = grad_f(x)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        \n",
    "        # L-BFGS direction computation\n",
    "        if len(s_list) == 0:\n",
    "            direction = -grad\n",
    "        else:\n",
    "            direction = -lbfgs_two_loop(s_list, y_list, grad, m)\n",
    "        \n",
    "        # Line search\n",
    "        alpha = armijo_line_search(f, grad_f, x, direction)\n",
    "        \n",
    "        # Update\n",
    "        x_new = x + alpha * direction\n",
    "        grad_new = grad_f(x_new)\n",
    "        \n",
    "        # Store history\n",
    "        s = x_new - x\n",
    "        y = grad_new - grad\n",
    "        \n",
    "        if np.dot(s, y) > 1e-10:\n",
    "            s_list.append(s)\n",
    "            y_list.append(y)\n",
    "            \n",
    "            # Memory limit\n",
    "            if len(s_list) > m:\n",
    "                s_list.pop(0)\n",
    "                y_list.pop(0)\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_new))\n",
    "        history['step_size'].append(alpha)\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "        grad = grad_new\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ak4uut29ft",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(f, grad_f, x0, learning_rate=0.01, beta=0.9, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"Momentum gradient descent\"\"\"\n",
    "    x = x0.copy()\n",
    "    v = np.zeros_like(x)\n",
    "    \n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        \n",
    "        v = beta * v + learning_rate * grad\n",
    "        x_new = x - v\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_f(x_new)))\n",
    "        history['step_size'].append(np.linalg.norm(v))\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jkvt9bc6gnp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov(f, grad_f, x0, learning_rate=0.01, beta=0.9, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"Nesterov accelerated gradient descent\"\"\"\n",
    "    x = x0.copy()\n",
    "    v = np.zeros_like(x)\n",
    "    \n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Gradient at anticipated future position\n",
    "        x_future = x - beta * v\n",
    "        grad = grad_f(x_future)\n",
    "        grad_norm = np.linalg.norm(grad_f(x))\n",
    "        \n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        \n",
    "        v = beta * v + learning_rate * grad\n",
    "        x_new = x - v\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_f(x_new)))\n",
    "        history['step_size'].append(np.linalg.norm(v))\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vjkkuhxofm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm comparison on quadratic functions\n",
    "print(\"9.3 Optimization Algorithms Comparison (Quadratic Functions)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "x0 = np.array([4.0, -3.0])\n",
    "\n",
    "# Verify all required functions are available\n",
    "try:\n",
    "    # Test line search functions\n",
    "    test_direction = np.array([-1.0, 0.0])\n",
    "    armijo_line_search(f1, grad_f1, x0, test_direction)\n",
    "    print(\"Line search functions are available\")\n",
    "    \n",
    "    # BFGS vs L-BFGS comparison (well-conditioned function)\n",
    "    print(f\"\\nQuasi-Newton methods comparison on condition number {np.linalg.cond(Q1):.1f} function:\")\n",
    "    x_bfgs, hist_bfgs = bfgs(f1, grad_f1, x0, max_iter=50)\n",
    "    x_lbfgs, hist_lbfgs = lbfgs(f1, grad_f1, x0, max_iter=50)\n",
    "\n",
    "    print(f\"  BFGS:   Final error {np.linalg.norm(x_bfgs - analytical_opt1):.2e}, Iterations {len(hist_bfgs['f'])-1:3d}\")\n",
    "    print(f\"  L-BFGS: Final error {np.linalg.norm(x_lbfgs - analytical_opt1):.2e}, Iterations {len(hist_lbfgs['f'])-1:3d}\")\n",
    "\n",
    "    # Momentum methods comparison\n",
    "    print(f\"\\nMomentum methods comparison on condition number {np.linalg.cond(Q1):.1f} function:\")\n",
    "    x_gd, hist_gd = gradient_descent(f1, grad_f1, x0, learning_rate=0.1, max_iter=100)\n",
    "    x_mom, hist_mom = momentum(f1, grad_f1, x0, learning_rate=0.1, beta=0.9, max_iter=100)  \n",
    "    x_nest, hist_nest = nesterov(f1, grad_f1, x0, learning_rate=0.1, beta=0.9, max_iter=100)\n",
    "\n",
    "    print(f\"  GD:        Final error {np.linalg.norm(x_gd - analytical_opt1):.2e}, Iterations {len(hist_gd['f'])-1:3d}\")\n",
    "    print(f\"  Momentum:  Final error {np.linalg.norm(x_mom - analytical_opt1):.2e}, Iterations {len(hist_mom['f'])-1:3d}\")\n",
    "    print(f\"  Nesterov:  Final error {np.linalg.norm(x_nest - analytical_opt1):.2e}, Iterations {len(hist_nest['f'])-1:3d}\")\n",
    "\n",
    "    # Store results for visualization\n",
    "    results_quadratic = {\n",
    "        'BFGS': (x_bfgs, hist_bfgs),\n",
    "        'L-BFGS': (x_lbfgs, hist_lbfgs),\n",
    "        'GD': (x_gd, hist_gd),\n",
    "        'Momentum': (x_mom, hist_mom),\n",
    "        'Nesterov': (x_nest, hist_nest)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nAlgorithm comparison completed successfully\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"Error: Missing function - {e}\")\n",
    "    print(\"Please ensure all previous cells have been executed in order.\")\n",
    "    # Create empty results for safety\n",
    "    results_quadratic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6blpr84nlqn",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:58:05.266926Z",
     "iopub.status.busy": "2025-08-28T16:58:05.266926Z",
     "iopub.status.idle": "2025-08-28T16:58:05.272635Z",
     "shell.execute_reply": "2025-08-28T16:58:05.272635Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adaptive learning rate methods and logistic regression experiments\n",
    "\n",
    "def adagrad(f, grad_f, x0, learning_rate=0.01, eps=1e-8, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"Adagrad algorithm\"\"\"\n",
    "    x = x0.copy()\n",
    "    G = np.zeros_like(x)\n",
    "    \n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        \n",
    "        G += grad * grad\n",
    "        adapted_lr = learning_rate / (np.sqrt(G) + eps)\n",
    "        x_new = x - adapted_lr * grad\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_f(x_new)))\n",
    "        history['step_size'].append(np.mean(adapted_lr))\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1im0b58t",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(f, grad_f, x0, learning_rate=0.01, beta=0.9, eps=1e-8, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"RMSProp algorithm\"\"\"\n",
    "    x = x0.copy()\n",
    "    v = np.zeros_like(x)\n",
    "    \n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        \n",
    "        v = beta * v + (1 - beta) * grad * grad\n",
    "        adapted_lr = learning_rate / (np.sqrt(v) + eps)\n",
    "        x_new = x - adapted_lr * grad\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_f(x_new)))\n",
    "        history['step_size'].append(np.mean(adapted_lr))\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "def adadelta(f, grad_f, x0, beta=0.95, eps=1e-6, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"Adadelta algorithm\"\"\"\n",
    "    x = x0.copy()\n",
    "    v = np.zeros_like(x)\n",
    "    u = np.zeros_like(x)\n",
    "    \n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        \n",
    "        v = beta * v + (1 - beta) * grad * grad\n",
    "        delta_x = -np.sqrt(u + eps) / np.sqrt(v + eps) * grad\n",
    "        u = beta * u + (1 - beta) * delta_x * delta_x\n",
    "        x_new = x + delta_x\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_f(x_new)))\n",
    "        history['step_size'].append(np.linalg.norm(delta_x))\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "def adam(f, grad_f, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"Adam algorithm\"\"\"\n",
    "    x = x0.copy()\n",
    "    m = np.zeros_like(x)\n",
    "    v = np.zeros_like(x)\n",
    "    \n",
    "    history = {\n",
    "        'x': [x.copy()], \n",
    "        'f': [f(x)], \n",
    "        'grad_norm': [np.linalg.norm(grad_f(x))],\n",
    "        'step_size': [],\n",
    "        'iteration': [0],\n",
    "        'time': [0]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        \n",
    "        t = i + 1\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * grad * grad\n",
    "        \n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        \n",
    "        step = learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n",
    "        x_new = x - step\n",
    "        \n",
    "        current_time = time.time() - start_time\n",
    "        history['x'].append(x_new.copy())\n",
    "        history['f'].append(f(x_new))\n",
    "        history['grad_norm'].append(np.linalg.norm(grad_f(x_new)))\n",
    "        history['step_size'].append(np.linalg.norm(step))\n",
    "        history['iteration'].append(i+1)\n",
    "        history['time'].append(current_time)\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0uxgmqwp8wga",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic logistic regression data generation (small dataset)\n",
    "def create_logistic_regression_data(n_samples=100, n_features=5):\n",
    "    \"\"\"Generate synthetic data for logistic regression\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    true_w = np.random.randn(n_features)\n",
    "    y = (X @ true_w + 0.1 * np.random.randn(n_samples) > 0).astype(float)\n",
    "    return X, y, true_w\n",
    "\n",
    "def logistic_loss_and_grad(w, X, y):\n",
    "    \"\"\"Logistic loss function and gradient\"\"\"\n",
    "    z = X @ w\n",
    "    # Clipping for numerical stability\n",
    "    z = np.clip(z, -500, 500)\n",
    "    \n",
    "    # Logistic loss\n",
    "    loss = np.mean(np.log(1 + np.exp(-y * z)))\n",
    "    \n",
    "    # Gradient \n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    grad = -X.T @ (y * (1 - sigmoid)) / len(y)\n",
    "    \n",
    "    return loss, grad\n",
    "\n",
    "# Generate logistic regression data\n",
    "X, y, true_w = create_logistic_regression_data(100, 5)\n",
    "y = 2 * y - 1  # {0,1} -> {-1,1}\n",
    "\n",
    "def logistic_f(w):\n",
    "    loss, _ = logistic_loss_and_grad(w, X, y)\n",
    "    return loss\n",
    "\n",
    "def logistic_grad_f(w):\n",
    "    _, grad = logistic_loss_and_grad(w, X, y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utvtx2vdi9o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive learning rate methods comparison (logistic regression)\n",
    "print(\"\\nAdaptive Learning Rate Methods Comparison (Logistic Regression)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "w0 = np.random.randn(5) * 0.1\n",
    "print(f\"Initial loss: {logistic_f(w0):.6f}\")\n",
    "\n",
    "# Verify adaptive functions are available\n",
    "try:\n",
    "    # Test functions exist\n",
    "    test_grad = np.array([0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "    adagrad(lambda x: np.sum(x**2), lambda x: 2*x, w0*0.1, max_iter=1)\n",
    "    print(\"Adaptive learning rate functions are available\")\n",
    "    \n",
    "    adaptive_methods = [\n",
    "        ('Adagrad', lambda: adagrad(logistic_f, logistic_grad_f, w0, learning_rate=0.1, max_iter=500)),\n",
    "        ('RMSProp', lambda: rmsprop(logistic_f, logistic_grad_f, w0, learning_rate=0.01, max_iter=500)),  \n",
    "        ('Adadelta', lambda: adadelta(logistic_f, logistic_grad_f, w0, max_iter=500)),\n",
    "        ('Adam', lambda: adam(logistic_f, logistic_grad_f, w0, learning_rate=0.01, max_iter=500)),\n",
    "    ]\n",
    "\n",
    "    results_adaptive = {}\n",
    "    for name, method in adaptive_methods:\n",
    "        w_opt, history = method()\n",
    "        results_adaptive[name] = (w_opt, history)\n",
    "        final_loss = logistic_f(w_opt)\n",
    "        print(f\"  {name:10}: Final loss {final_loss:.6f}, Iterations {len(history['f'])-1:3d}\")\n",
    "\n",
    "    print(f\"\\nReference - True weights: {true_w}\")\n",
    "    for name, (w_opt, _) in results_adaptive.items():\n",
    "        error = np.linalg.norm(w_opt - true_w)\n",
    "        print(f\"  {name:10}: Weight error {error:.6f}\")\n",
    "        \n",
    "    print(\"\\nAdaptive learning rate comparison completed successfully\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"Error: Missing function - {e}\")\n",
    "    print(\"Please ensure all previous cells have been executed in order.\")\n",
    "    # Create empty results for safety\n",
    "    results_adaptive = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ntg7vsjnqkb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Adaptive learning rate methods comparison\n",
    "\n",
    "def plot_adaptive_methods_comparison():\n",
    "    \"\"\"Visualize adaptive learning rate methods performance\"\"\"\n",
    "    \n",
    "    # Check if results are available\n",
    "    if 'results_adaptive' not in globals() or not results_adaptive:\n",
    "        print(\"Error: Adaptive algorithm results not available. Please run the adaptive methods comparison cell first.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot 0: Loss convergence comparison\n",
    "    ax = axes[0]\n",
    "    colors_adaptive = ['red', 'blue', 'green', 'purple']\n",
    "    \n",
    "    for i, (method_name, (w_opt, hist)) in enumerate(results_adaptive.items()):\n",
    "        loss_values = hist['f']\n",
    "        ax.plot(loss_values, color=colors_adaptive[i], linewidth=2, \n",
    "                label=method_name, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Loss Convergence: Adaptive Methods')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Plot 1: Gradient norm evolution\n",
    "    ax = axes[1]\n",
    "    \n",
    "    for i, (method_name, (w_opt, hist)) in enumerate(results_adaptive.items()):\n",
    "        grad_norms = hist['grad_norm']\n",
    "        ax.semilogy(grad_norms, color=colors_adaptive[i], linewidth=2, \n",
    "                   label=method_name, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('||∇f(w)||')\n",
    "    ax.set_title('Gradient Norm Evolution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Step size evolution\n",
    "    ax = axes[2]\n",
    "    \n",
    "    for i, (method_name, (w_opt, hist)) in enumerate(results_adaptive.items()):\n",
    "        step_sizes = hist['step_size']\n",
    "        if step_sizes and len(step_sizes) > 1:\n",
    "            ax.plot(step_sizes, color=colors_adaptive[i], linewidth=2, \n",
    "                   label=method_name, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Step Size')\n",
    "    ax.set_title('Adaptive Step Size Evolution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Plot 3: Final performance comparison\n",
    "    ax = axes[3]\n",
    "    \n",
    "    method_names = list(results_adaptive.keys())\n",
    "    final_losses = [results_adaptive[name][1]['f'][-1] for name in method_names]\n",
    "    iterations = [len(results_adaptive[name][1]['f'])-1 for name in method_names]\n",
    "    \n",
    "    x_pos = np.arange(len(method_names))\n",
    "    bars = ax.bar(x_pos, final_losses, color=colors_adaptive[:len(method_names)], alpha=0.7)\n",
    "    \n",
    "    # Display iteration count on top of bars\n",
    "    for i, (bar, iters) in enumerate(zip(bars, iterations)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + height*0.05,\n",
    "                f'{iters}it', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Algorithm')\n",
    "    ax.set_ylabel('Final Loss')\n",
    "    ax.set_title('Final Loss Comparison')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(method_names, rotation=45)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the visualization function\n",
    "plot_adaptive_methods_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bodh0uh3ig",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:58:05.275642Z",
     "iopub.status.busy": "2025-08-28T16:58:05.275642Z",
     "iopub.status.idle": "2025-08-28T16:58:05.284324Z",
     "shell.execute_reply": "2025-08-28T16:58:05.284324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 1: Contour plots with optimization paths for quadratic functions\n",
    "\n",
    "def plot_optimization_paths():\n",
    "    \"\"\"Visualize optimization paths on quadratic functions\"\"\"\n",
    "    \n",
    "    # Check if results are available\n",
    "    if 'results_quadratic' not in globals() or not results_quadratic:\n",
    "        print(\"Error: Algorithm results not available. Please run the algorithm comparison cell first.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Prepare visualization for condition number 10 function\n",
    "    x_range = np.linspace(-6, 6, 100)\n",
    "    y_range = np.linspace(-6, 6, 100)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z1 = 0.5 * (Q1[0,0] * X**2 + Q1[1,1] * Y**2) + b1[0] * X + b1[1] * Y\n",
    "    \n",
    "    # Prepare visualization for condition number 1000 function  \n",
    "    x_range2 = np.linspace(-4, 4, 100)\n",
    "    y_range2 = np.linspace(-2, 2, 100)\n",
    "    X2, Y2 = np.meshgrid(x_range2, y_range2)\n",
    "    Z2 = 0.5 * (Q2[0,0] * X2**2 + Q2[1,1] * Y2**2) + b2[0] * X2 + b2[1] * Y2\n",
    "    \n",
    "    # Starting points\n",
    "    start_points = [np.array([4.0, -3.0]), np.array([3.0, -2.0])]\n",
    "    \n",
    "    # Plot 0: Condition number 10, various algorithms\n",
    "    ax = axes[0]\n",
    "    ax.contour(X, Y, Z1, levels=20, colors='gray', alpha=0.6, linewidths=0.8)\n",
    "    ax.contourf(X, Y, Z1, levels=20, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    methods_to_plot = ['GD', 'BFGS', 'Momentum', 'Nesterov']\n",
    "    colors_plot = ['red', 'blue', 'green', 'purple']\n",
    "    markers = ['o', 's', '^', 'D']\n",
    "    \n",
    "    for i, method in enumerate(methods_to_plot):\n",
    "        if method in results_quadratic:\n",
    "            path = np.array(results_quadratic[method][1]['x'])\n",
    "            # Limit path length for visualization\n",
    "            path = path[:min(50, len(path))]\n",
    "            ax.plot(path[:, 0], path[:, 1], color=colors_plot[i], marker=markers[i], \n",
    "                   markersize=3, alpha=0.8, linewidth=2, label=method)\n",
    "    \n",
    "    ax.plot(analytical_opt1[0], analytical_opt1[1], 'k*', markersize=12, label='Optimum')\n",
    "    ax.plot(start_points[0][0], start_points[0][1], 'ko', markersize=8, label='Start')\n",
    "    ax.set_title(f'Optimization Paths (κ={np.linalg.cond(Q1):.0f})', fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Plot 1: Condition number 1000, gradient descent comparison\n",
    "    ax = axes[1]\n",
    "    ax.contour(X2, Y2, Z2, levels=20, colors='gray', alpha=0.6, linewidths=0.8)\n",
    "    ax.contourf(X2, Y2, Z2, levels=20, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    # Run gradient descent on ill-conditioned function for visualization\n",
    "    try:\n",
    "        x_gd_ill, hist_gd_ill = gradient_descent(f2, grad_f2, start_points[1], learning_rate=0.001, max_iter=200)\n",
    "        path_ill = np.array(hist_gd_ill['x'])\n",
    "        path_ill = path_ill[:min(100, len(path_ill))]  # Limit for visualization\n",
    "        \n",
    "        ax.plot(path_ill[:, 0], path_ill[:, 1], 'red', marker='o', markersize=2, \n",
    "               alpha=0.7, linewidth=1, label='GD')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not generate ill-conditioned visualization: {e}\")\n",
    "    \n",
    "    ax.plot(analytical_opt2[0], analytical_opt2[1], 'k*', markersize=12, label='Optimum')\n",
    "    ax.plot(start_points[1][0], start_points[1][1], 'ko', markersize=8, label='Start')\n",
    "    ax.set_title(f'Ill-Conditioned Function (κ={np.linalg.cond(Q2):.0f})', fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Plot 2: Step size visualization\n",
    "    ax = axes[2]\n",
    "    step_plotted = False\n",
    "    for i, method in enumerate(['GD', 'BFGS', 'Momentum', 'Nesterov']):\n",
    "        if method in results_quadratic:\n",
    "            step_sizes = results_quadratic[method][1]['step_size']\n",
    "            if step_sizes:  # Check if step_size data exists\n",
    "                step_data = step_sizes[:min(50, len(step_sizes))]\n",
    "                if len(step_data) > 1:  # Only plot if we have meaningful data\n",
    "                    ax.plot(step_data, color=colors_plot[i], \n",
    "                           linewidth=2, label=method, alpha=0.8)\n",
    "                    step_plotted = True\n",
    "    \n",
    "    if step_plotted:\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Step Size')\n",
    "        ax.set_title('Step Size Evolution')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Step size data\\nnot available', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Step Size Evolution')\n",
    "    \n",
    "    # Plot 3: Function value convergence curves (condition number 10)\n",
    "    ax = axes[3]\n",
    "    for i, method in enumerate(methods_to_plot):\n",
    "        if method in results_quadratic:\n",
    "            f_values = results_quadratic[method][1]['f']\n",
    "            # Difference from optimal value\n",
    "            f_diff = np.array(f_values) - f1(analytical_opt1)\n",
    "            f_diff = np.maximum(f_diff, 1e-16)  # Clipping for log\n",
    "            ax.semilogy(f_diff, color=colors_plot[i], linewidth=2, label=method)\n",
    "    \n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('f(x) - f*')\n",
    "    ax.set_title('Convergence: f(x) - f*')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Gradient norm evolution\n",
    "    ax = axes[4]\n",
    "    for i, method in enumerate(methods_to_plot):\n",
    "        if method in results_quadratic:\n",
    "            grad_norms = results_quadratic[method][1]['grad_norm']\n",
    "            ax.semilogy(grad_norms, color=colors_plot[i], linewidth=2, label=method)\n",
    "    \n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('||∇f(x)||')\n",
    "    ax.set_title('Gradient Norm Evolution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Computation time comparison\n",
    "    ax = axes[5]\n",
    "    methods_time = []\n",
    "    times = []\n",
    "    iterations = []\n",
    "    \n",
    "    for method_name in methods_to_plot:\n",
    "        if method_name in results_quadratic:\n",
    "            hist = results_quadratic[method_name][1]\n",
    "            methods_time.append(method_name)\n",
    "            times.append(hist['time'][-1])\n",
    "            iterations.append(len(hist['f'])-1)\n",
    "    \n",
    "    if methods_time:  # Only create plot if we have data\n",
    "        x_pos = np.arange(len(methods_time))\n",
    "        bars = ax.bar(x_pos, times, color=colors_plot[:len(methods_time)], alpha=0.7)\n",
    "        \n",
    "        # Display iteration count on top of bars\n",
    "        for i, (bar, iters) in enumerate(zip(bars, iterations)):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.05,\n",
    "                    f'{iters}it', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        ax.set_xlabel('Algorithm')\n",
    "        ax.set_ylabel('Time (seconds)')\n",
    "        ax.set_title('Computation Time Comparison')\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(methods_time, rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Timing data\\nnot available', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Computation Time Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the visualization function\n",
    "plot_optimization_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38qm2o8xfyr",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:58:05.287330Z",
     "iopub.status.busy": "2025-08-28T16:58:05.287330Z",
     "iopub.status.idle": "2025-08-28T16:58:06.088915Z",
     "shell.execute_reply": "2025-08-28T16:58:06.087907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 2: Learning rate and convergence speed analysis\n",
    "\n",
    "def plot_learning_rate_analysis():\n",
    "    \"\"\"Detailed analysis of learning rate impact and convergence speed\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Various learning rates experiment\n",
    "    learning_rates = [0.05, 0.1, 0.15, 0.2]\n",
    "    x0 = np.array([3.0, -2.0])\n",
    "    \n",
    "    # Plot 0: Convergence curves by learning rate (condition number 10)\n",
    "    ax = axes[0, 0]\n",
    "    for i, lr in enumerate(learning_rates):\n",
    "        try:\n",
    "            x_opt, hist = gradient_descent(f1, grad_f1, x0, learning_rate=lr, max_iter=100)\n",
    "            f_diff = np.array(hist['f']) - f1(analytical_opt1)\n",
    "            f_diff = np.maximum(f_diff, 1e-16)\n",
    "            ax.semilogy(f_diff, color=colors[i], linewidth=2, label=f'α={lr}')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not plot learning rate {lr}: {e}\")\n",
    "    \n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('f(x) - f*')\n",
    "    ax.set_title('Learning Rate Effect (κ=10)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 1: Convergence curves by learning rate (condition number 1000)\n",
    "    ax = axes[0, 1]\n",
    "    learning_rates_bad = [0.0005, 0.001, 0.0015, 0.002]\n",
    "    for i, lr in enumerate(learning_rates_bad):\n",
    "        try:\n",
    "            x_opt, hist = gradient_descent(f2, grad_f2, x0, learning_rate=lr, max_iter=1000)\n",
    "            f_diff = np.array(hist['f']) - f2(analytical_opt2)\n",
    "            f_diff = np.maximum(f_diff, 1e-16)\n",
    "            # Length limit\n",
    "            max_len = min(500, len(f_diff))\n",
    "            ax.semilogy(f_diff[:max_len], color=colors[i], linewidth=2, label=f'α={lr}')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not plot learning rate {lr}: {e}\")\n",
    "    \n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('f(x) - f*')  \n",
    "    ax.set_title('Learning Rate Effect (κ=1000)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Step size changes (Armijo vs Wolfe)\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    try:\n",
    "        # Armijo vs Wolfe step size comparison\n",
    "        x_armijo, hist_armijo = gradient_descent_with_line_search(f1, grad_f1, x0, 'armijo', max_iter=50)\n",
    "        x_wolfe, hist_wolfe = gradient_descent_with_line_search(f1, grad_f1, x0, 'wolfe', max_iter=50)\n",
    "        \n",
    "        ax.plot(hist_armijo['step_size'], 'b-', linewidth=2, label='Armijo', alpha=0.8)\n",
    "        ax.plot(hist_wolfe['step_size'], 'r-', linewidth=2, label='Wolfe', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Step Size (α)')\n",
    "        ax.set_title('Line Search Comparison')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not generate line search comparison: {e}\")\n",
    "        ax.text(0.5, 0.5, 'Line search data\\nnot available', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Line Search Comparison')\n",
    "    \n",
    "    # Plot 3: Algorithm performance comparison\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Check if we have quadratic results\n",
    "    if 'results_quadratic' in globals() and results_quadratic:\n",
    "        methods_time = []\n",
    "        times = []\n",
    "        iterations = []\n",
    "        \n",
    "        for method_name in ['GD', 'BFGS', 'L-BFGS', 'Momentum', 'Nesterov']:\n",
    "            if method_name in results_quadratic:\n",
    "                hist = results_quadratic[method_name][1]\n",
    "                methods_time.append(method_name)\n",
    "                times.append(hist['time'][-1])\n",
    "                iterations.append(len(hist['f'])-1)\n",
    "        \n",
    "        if methods_time:\n",
    "            x_pos = np.arange(len(methods_time))\n",
    "            bars = ax.bar(x_pos, times, color=colors[:len(methods_time)], alpha=0.7)\n",
    "            \n",
    "            # Display iteration count on top of bars\n",
    "            for i, (bar, iters) in enumerate(zip(bars, iterations)):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + height*0.05,\n",
    "                        f'{iters}it', ha='center', va='bottom', fontsize=9)\n",
    "            \n",
    "            ax.set_xlabel('Algorithm')\n",
    "            ax.set_ylabel('Time (seconds)')\n",
    "            ax.set_title('Performance Comparison')\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(methods_time, rotation=45)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Performance data\\nnot available', ha='center', va='center', \n",
    "                    transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_title('Performance Comparison')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Performance data\\nnot available', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Performance Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the visualization function\n",
    "plot_learning_rate_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g65vpg4v2l",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:58:06.177641Z",
     "iopub.status.busy": "2025-08-28T16:58:06.177641Z",
     "iopub.status.idle": "2025-08-28T16:58:07.340144Z",
     "shell.execute_reply": "2025-08-28T16:58:07.340144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance summary table and key observations\n",
    "\n",
    "def create_summary_table():\n",
    "    \"\"\"Generate optimization algorithm performance summary table\"\"\"\n",
    "    \n",
    "    # Check if results are available\n",
    "    if 'results_quadratic' not in globals() or not results_quadratic:\n",
    "        print(\"Error: Quadratic algorithm results not available\")\n",
    "        return [], []\n",
    "    \n",
    "    if 'results_adaptive' not in globals() or not results_adaptive:\n",
    "        print(\"Error: Adaptive algorithm results not available\") \n",
    "        return [], []\n",
    "    \n",
    "    # Collect quadratic function results (condition number 10)\n",
    "    summary_data = []\n",
    "    \n",
    "    for method_name, (x_opt, hist) in results_quadratic.items():\n",
    "        final_f = hist['f'][-1]\n",
    "        final_grad_norm = hist['grad_norm'][-1]\n",
    "        iterations = len(hist['f']) - 1\n",
    "        total_time = hist['time'][-1]\n",
    "        avg_step = np.mean(hist['step_size']) if hist['step_size'] else 0\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Algorithm': method_name,\n",
    "            'Final f': f\"{final_f:.2e}\",\n",
    "            'Final ||∇f||': f\"{final_grad_norm:.2e}\", \n",
    "            'Iterations': iterations,\n",
    "            'Time (s)': f\"{total_time:.4f}\",\n",
    "            'Avg Step': f\"{avg_step:.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Add adaptive learning rate results (logistic regression)\n",
    "    adaptive_data = []\n",
    "    for method_name, (w_opt, hist) in results_adaptive.items():\n",
    "        final_loss = hist['f'][-1]\n",
    "        final_grad_norm = hist['grad_norm'][-1] \n",
    "        iterations = len(hist['f']) - 1\n",
    "        total_time = hist['time'][-1]\n",
    "        avg_step = np.mean(hist['step_size']) if hist['step_size'] else 0\n",
    "        \n",
    "        adaptive_data.append({\n",
    "            'Algorithm': method_name,\n",
    "            'Final Loss': f\"{final_loss:.4f}\",\n",
    "            'Final ||∇f||': f\"{final_grad_norm:.2e}\",\n",
    "            'Iterations': iterations,\n",
    "            'Time (s)': f\"{total_time:.4f}\",\n",
    "            'Avg Step': f\"{avg_step:.4f}\"\n",
    "        })\n",
    "    \n",
    "    return summary_data, adaptive_data\n",
    "\n",
    "# Create and display summary tables\n",
    "try:\n",
    "    summary_quadratic, summary_adaptive = create_summary_table()\n",
    "    \n",
    "    if summary_quadratic and summary_adaptive:\n",
    "        print(\"Optimization Algorithm Performance Summary\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Quadratic function summary table\n",
    "        print(f\"\\nQuadratic Function Optimization Results (condition number {np.linalg.cond(Q1):.0f})\")\n",
    "        print(\"-\"*80)\n",
    "        df_quad = pd.DataFrame(summary_quadratic)\n",
    "        df_quad_sorted = df_quad.sort_values('Iterations').reset_index(drop=True)\n",
    "        print(df_quad_sorted.to_string(index=False))\n",
    "\n",
    "        # Adaptive learning rate summary table \n",
    "        print(f\"\\nAdaptive Learning Rate Methods Results (Logistic Regression)\")\n",
    "        print(\"-\"*80)\n",
    "        df_adaptive = pd.DataFrame(summary_adaptive)\n",
    "        df_adaptive_sorted = df_adaptive.sort_values('Iterations').reset_index(drop=True)\n",
    "        print(df_adaptive_sorted.to_string(index=False))\n",
    "\n",
    "        # Condition number impact comparison\n",
    "        print(f\"\\nCondition Number Impact Comparison\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"Condition number {np.linalg.cond(Q1):.0f} vs {np.linalg.cond(Q2):.0f}\")\n",
    "\n",
    "        # Check if hist_gd2 exists for comparison\n",
    "        if 'hist_gd2' in globals() and 'hist_gd' in globals():\n",
    "            gd_cond10_iters = len(hist_gd['f']) - 1\n",
    "            gd_cond1000_iters = len(hist_gd2['f']) - 1\n",
    "            print(f\"Gradient descent iterations: {gd_cond10_iters} vs {gd_cond1000_iters}\")\n",
    "            print(f\"Convergence speed difference: {gd_cond1000_iters/gd_cond10_iters:.1f}x slower\")\n",
    "        else:\n",
    "            print(\"Note: Detailed condition number comparison requires running the full algorithm comparison.\")\n",
    "\n",
    "        print(\"\\nPerformance summary completed successfully\")\n",
    "    else:\n",
    "        print(\"Error: Could not generate performance summary tables\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating performance summary: {e}\")\n",
    "    print(\"Please ensure all algorithm comparison cells have been executed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "epy80gu11rj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master execution cell: Run all analysis and visualizations\n",
    "\n",
    "def run_complete_analysis():\n",
    "    \"\"\"Execute complete optimization algorithm analysis and visualization\"\"\"\n",
    "    \n",
    "    print(\"Starting complete optimization algorithm analysis...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Verify basic setup\n",
    "    try:\n",
    "        print(\"Step 1: Verifying basic functions...\")\n",
    "        test_f = f1(np.array([1.0, 1.0]))\n",
    "        print(f\"Quadratic functions working (test: f1([1,1]) = {test_f:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Basic function error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Step 2: Test line search methods\n",
    "    try:\n",
    "        print(\"\\nStep 2: Testing line search methods...\")\n",
    "        test_line_search_methods()\n",
    "    except Exception as e:\n",
    "        print(f\"Line search test error: {e}\")\n",
    "    \n",
    "    # Step 3: Run algorithm comparisons\n",
    "    try:\n",
    "        print(\"\\nStep 3: Running algorithm comparisons...\")\n",
    "        print(\"This may take a moment...\")\n",
    "        \n",
    "        # Algorithm comparison code\n",
    "        x0 = np.array([4.0, -3.0])\n",
    "\n",
    "        # BFGS vs L-BFGS comparison\n",
    "        print(\"Running quasi-Newton methods...\")\n",
    "        x_bfgs, hist_bfgs = bfgs(f1, grad_f1, x0, max_iter=50)\n",
    "        x_lbfgs, hist_lbfgs = lbfgs(f1, grad_f1, x0, max_iter=50)\n",
    "\n",
    "        # Momentum methods comparison  \n",
    "        print(\"Running momentum methods...\")\n",
    "        x_gd, hist_gd = gradient_descent(f1, grad_f1, x0, learning_rate=0.1, max_iter=100)\n",
    "        x_mom, hist_mom = momentum(f1, grad_f1, x0, learning_rate=0.1, beta=0.9, max_iter=100)  \n",
    "        x_nest, hist_nest = nesterov(f1, grad_f1, x0, learning_rate=0.1, beta=0.9, max_iter=100)\n",
    "\n",
    "        # Store results\n",
    "        globals()['results_quadratic'] = {\n",
    "            \"BFGS\": (x_bfgs, hist_bfgs),\n",
    "            \"L-BFGS\": (x_lbfgs, hist_lbfgs), \n",
    "            \"GD\": (x_gd, hist_gd),\n",
    "            \"Momentum\": (x_mom, hist_mom),\n",
    "            \"Nesterov\": (x_nest, hist_nest)\n",
    "        }\n",
    "\n",
    "        print(\"Algorithm comparison completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Algorithm comparison had issues: {e}\")\n",
    "    \n",
    "    # Step 4: Run adaptive methods\n",
    "    try:\n",
    "        print(\"\\nStep 4: Running adaptive learning rate methods...\")\n",
    "        w0 = np.random.randn(5) * 0.1\n",
    "        \n",
    "        print(\"Running Adagrad...\")\n",
    "        w_adagrad, hist_adagrad = adagrad(logistic_f, logistic_grad_f, w0, learning_rate=0.1, max_iter=500)\n",
    "        \n",
    "        print(\"Running RMSProp...\")  \n",
    "        w_rmsprop, hist_rmsprop = rmsprop(logistic_f, logistic_grad_f, w0, learning_rate=0.01, max_iter=500)\n",
    "        \n",
    "        print(\"Running Adam...\")\n",
    "        w_adam, hist_adam = adam(logistic_f, logistic_grad_f, w0, learning_rate=0.01, max_iter=500)\n",
    "        \n",
    "        print(\"Running Adadelta...\")\n",
    "        w_adadelta, hist_adadelta = adadelta(logistic_f, logistic_grad_f, w0, max_iter=500)\n",
    "        \n",
    "        # Store adaptive results\n",
    "        globals()['results_adaptive'] = {\n",
    "            'Adagrad': (w_adagrad, hist_adagrad),\n",
    "            'RMSProp': (w_rmsprop, hist_rmsprop),  \n",
    "            'Adam': (w_adam, hist_adam),\n",
    "            'Adadelta': (w_adadelta, hist_adadelta),\n",
    "        }\n",
    "        \n",
    "        print(\"Adaptive methods completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Adaptive methods had issues: {e}\")\n",
    "    \n",
    "    # Step 5: Generate visualizations\n",
    "    print(\"\\nStep 5: Generating visualizations...\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Creating main optimization paths visualization...\")\n",
    "        plot_optimization_paths()\n",
    "        print(\"Main visualization completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Main visualization error: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Creating learning rate analysis...\")\n",
    "        plot_learning_rate_analysis()\n",
    "        print(\"Learning rate analysis completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Learning rate analysis error: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Creating adaptive methods comparison...\")\n",
    "        plot_adaptive_methods_comparison()\n",
    "        print(\"Adaptive methods visualization completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Adaptive methods visualization error: {e}\")\n",
    "    \n",
    "    # Step 6: Generate summary\n",
    "    print(\"\\nStep 6: Generating performance summary...\")\n",
    "    try:\n",
    "        summary_quadratic, summary_adaptive = create_summary_table()\n",
    "\n",
    "        if summary_quadratic and summary_adaptive:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Display tables...\n",
    "            df_quad = pd.DataFrame(summary_quadratic)\n",
    "            df_adaptive = pd.DataFrame(summary_adaptive)\n",
    "            \n",
    "            print(\"\\nQuadratic Function Results:\")\n",
    "            print(df_quad.to_string(index=False))\n",
    "            \n",
    "            print(\"\\nAdaptive Methods Results:\")  \n",
    "            print(df_adaptive.to_string(index=False))\n",
    "            \n",
    "            print(\"\\nAnalysis completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Summary generation error: {e}\")\n",
    "    \n",
    "    print(\"\\nComplete analysis finished!\")\n",
    "    return True\n",
    "\n",
    "# Run the complete analysis\n",
    "run_complete_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0l56ya9x8zrr",
   "metadata": {},
   "source": [
    "## Key Observations and Conclusions\n",
    "\n",
    "### Main Findings\n",
    "\n",
    "1. **Impact of condition number**: The larger the matrix condition number, the significantly slower the convergence of gradient descent. Functions with condition number 1000 showed convergence over 10 times slower.\n",
    "\n",
    "2. **Superiority of second-order methods**: BFGS and L-BFGS show very fast convergence on quadratic functions. BFGS theoretically converges in finite steps for quadratic functions.\n",
    "\n",
    "3. **Effect of momentum**: Momentum and Nesterov acceleration show improved convergence over gradient descent, but are not as effective as second-order methods on quadratic functions.\n",
    "\n",
    "4. **Adaptive learning rates**: Adam and RMSProp demonstrate stable performance on nonlinear problems like logistic regression.\n",
    "\n",
    "5. **Importance of line search**: Line search using Armijo and Wolfe conditions provides more stable and faster convergence than fixed learning rates.\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "- **Quadratic functions**: Use BFGS or L-BFGS\n",
    "- **Large-scale problems**: Use L-BFGS or Adam\n",
    "- **Neural network training**: Use Adam, RMSProp, or adaptive momentum\n",
    "- **Ill-conditioned problems**: Use quasi-Newton methods with line search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7g075fsf1",
   "metadata": {},
   "source": [
    "## Appendix: Nonlinear Example\n",
    "\n",
    "As a reference, we briefly present optimization experiments on a nonlinear test function.\n",
    "\n",
    "Test function: $f(x, y) = (1-x)^2 + 100(y-x^2)^2$\n",
    "\n",
    "This function is challenging to optimize due to its nonlinearity and narrow valley structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pivr8pyszec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendix: Nonlinear function experiment\n",
    "\n",
    "def nonlinear_test_function(x):\n",
    "    \"\"\"Nonlinear test function\"\"\"\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "def nonlinear_test_grad(x):\n",
    "    \"\"\"Gradient of nonlinear test function\"\"\"\n",
    "    grad_x = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
    "    grad_y = 200*(x[1] - x[0]**2)\n",
    "    return np.array([grad_x, grad_y])\n",
    "\n",
    "def nonlinear_test_hess(x):\n",
    "    \"\"\"Hessian of nonlinear test function\"\"\"\n",
    "    h11 = 2 + 1200*x[0]**2 - 400*x[1]\n",
    "    h12 = h21 = -400*x[0]\n",
    "    h22 = 200\n",
    "    return np.array([[h11, h12], [h21, h22]])\n",
    "\n",
    "print(\"Appendix: Nonlinear Function Optimization\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "x0_nonlinear = np.array([-1.0, 1.0])\n",
    "print(f\"Starting point: {x0_nonlinear}\")\n",
    "print(f\"Global optimum: [1.0, 1.0]\")\n",
    "print(f\"Optimal value: 0.0\")\n",
    "print()\n",
    "\n",
    "# Test with several algorithms\n",
    "methods_nonlinear = [\n",
    "    ('GD (lr=0.001)', lambda: gradient_descent(nonlinear_test_function, nonlinear_test_grad, x0_nonlinear, \n",
    "                                             learning_rate=0.001, max_iter=10000)),\n",
    "    ('BFGS', lambda: bfgs(nonlinear_test_function, nonlinear_test_grad, x0_nonlinear, max_iter=100)),\n",
    "    ('Adam (lr=0.01)', lambda: adam(nonlinear_test_function, nonlinear_test_grad, x0_nonlinear, \n",
    "                                  learning_rate=0.01, max_iter=1000)),\n",
    "]\n",
    "\n",
    "for name, method in methods_nonlinear:\n",
    "    x_opt, hist = method()\n",
    "    final_error = np.linalg.norm(x_opt - np.array([1.0, 1.0]))\n",
    "    final_f = nonlinear_test_function(x_opt)\n",
    "    iterations = len(hist['f']) - 1\n",
    "    print(f\"{name:15}: Final error {final_error:.3f}, Function value {final_f:.3f}, Iterations {iterations:4d}\")\n",
    "\n",
    "# Simple visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Nonlinear function contours\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "y_range = np.linspace(-1, 3, 100) \n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = (1 - X)**2 + 100*(Y - X**2)**2\n",
    "\n",
    "# Display contours on log scale\n",
    "ax1.contour(X, Y, np.log(Z + 1), levels=20, colors='gray', alpha=0.6)\n",
    "ax1.contourf(X, Y, np.log(Z + 1), levels=20, alpha=0.4, cmap='viridis')\n",
    "\n",
    "# Display BFGS path (recalculate)\n",
    "x_bfgs_nonlinear, hist_bfgs_nonlinear = bfgs(nonlinear_test_function, nonlinear_test_grad, x0_nonlinear, max_iter=50)\n",
    "path = np.array(hist_bfgs_nonlinear['x'])\n",
    "ax1.plot(path[:, 0], path[:, 1], 'ro-', markersize=4, linewidth=2, alpha=0.8, label='BFGS')\n",
    "ax1.plot(x0_nonlinear[0], x0_nonlinear[1], 'ko', markersize=8, label='Start')\n",
    "ax1.plot(1, 1, 'k*', markersize=12, label='Optimum')\n",
    "ax1.set_title('Nonlinear Function (log scale)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence curve\n",
    "ax2.semilogy(hist_bfgs_nonlinear['f'], 'b-', linewidth=2, label='BFGS')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Function Value (log scale)')\n",
    "ax2.set_title('BFGS Convergence on Nonlinear Function')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nExcellent performance of BFGS on nonlinear functions is confirmed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
